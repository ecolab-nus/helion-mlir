# Helion FX-to-MLIR Software Architecture

This document describes the software architecture of the `helion_fx_mlir` package, which converts Helion Device IR (FX graphs) to MLIR text representation.

## Overview

The `helion_fx_mlir` package uses an **instruction-driven** approach that walks Device IR FX graphs node-by-node. It leverages a **registry-based lowering system** to dispatch operations, integrating with **torch-mlir** for ATen operations.

When emitting MLIR:
1.  **Helion-specific FX nodes** (e.g., `load`, `store`, `loop`) are generated **in this project**.
2.  **Torch functions (`aten`)** are generated by the **torch-mlir project** as **torch dialect** ops.
3.  The generated MLIR is a mix of dialects including **`affine.parallel`, `affine.for`, `torch.aten.*`**, and `helion`.
4.  (Optional) The torch dialect ops can be further lowered to `linalg-on-tensors` using torch-mlir's backend pipelines.

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                        BoundKernel (Helion)                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────────┐  │
│  │ fake_args   │  │    env      │  │        host_function        │  │
│  │ (Tensors)   │  │(BlockSizes) │  │  └─> device_ir (DeviceIR)   │  │
│  └─────────────┘  └─────────────┘  │      └─> graphs [GraphInfo] │  │
│                                    │          └─> graph (FX)     │  │
│                                    └─────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         generate_mlir()                             │
│                        (helion_mlir.py)                             │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                    ┌───────────────┼───────────────┐
                    ▼               ▼               ▼
            ┌───────────┐   ┌──────────────┐   ┌──────────────┐
            │ IRVisitor │   │LoweringContext│  │  MLIRBuilder │
            │  (Walk)   │   │   (State)     │  │  (Emission)  │
            └───────────┘   └──────────────┘   └──────────────┘
                    │               │               │
        ┌───────────┴───────────────┴───────────────┴───────────┐
        │ Two Sources of MLIR Generation:                       │
        │ 1. Helion Nodes (this project)                        │
        │ 2. ATen Ops (torch-mlir dependency)                   │
        └───────────────────────────┬───────────────────────────┘
                                    ▼
                            ┌───────────────┐
                            │  MLIR Text    │
                            └───────────────┘
```

---

## Device IR Structure

Helion compiles kernels into an FX-based Device IR containing multiple graphs:

| Graph Type | Purpose | `block_ids` |
|------------|---------|-------------|
| `ForLoopGraphInfo` | Innermost loop body (typically reduction loops) | `[2]` (e.g., tile_k) |
| `RootGraphInfo` | Outer parallel structure and control flow | `None` |

### Device IR Nodes Reference

Key FX targets encountered in Device IR and their meanings:

| FX Target | Description | Example Node Name |
|-----------|-------------|-------------------|
| `helion.language.memory_ops.load` | Tile load from tensor | `load`, `load_1` |
| `helion.language.memory_ops.store` | Tile store to tensor | `store` |
| `helion.language._tracing_ops._host_tensor` | Reference to kernel argument tensor | `x`, `y`, `out` |
| `helion.language._tracing_ops._phi` | Loop-carried value (reduction) | `_phi` |
| `helion.language._tracing_ops._get_symnode` | Symbolic size reference | `block_size_0` |
| `helion.language._tracing_ops._for_loop` | Reduction loop marker | `_for_loop` |
| `helion.language.creation_ops.full` | Tensor initialization | `acc` |
| `aten.addmm.default` | Matrix multiply-add | `acc` |

### Python to Device IR Mapping Example

**Original Python Kernel:**
```python
@helion.kernel
def matmul(x: torch.Tensor, y: torch.Tensor, out: torch.Tensor) -> None:
    for tile_m, tile_n in hl.tile([x.size(0), y.size(1)]):
        acc = hl.zeros([tile_m, tile_n], dtype=torch.float32)
        for tile_k in hl.tile(x.size(1)):
            x_tile = x[tile_m, tile_k]  # load
            y_tile = y[tile_k, tile_n]  # load_1
            acc = torch.addmm(acc, x_tile, y_tile)  # acc
        out[tile_m, tile_n] = acc  # store
```

**Device IR (ForLoopGraphInfo)** - Inner `tile_k` loop:
```
call_function  x               _host_tensor    ('x',)
call_function  load            load            (x, [sym_size_int, block_size_2], ...)
call_function  y               _host_tensor    ('y',)
call_function  load_1          load            (y, [block_size_2, sym_size_int_1], ...)
call_function  acc             aten.addmm      (_new_var, load, load_1)
```

**Device IR (RootGraphInfo)** - Outer control flow:
```
call_function  block_size_0    _get_symnode    ('block_size_0',)
call_function  block_size_1    _get_symnode    ('block_size_1',)
call_function  acc             full            ([block_size_0, block_size_1], ...)
call_function  _for_loop       _for_loop       (0, [0], [x_size1], [acc])
call_function  _phi            _phi            (acc, getitem)
call_function  out             _host_tensor    ('out',)
call_function  store           store           (out, [...], _phi, ...)
```

---

## Package Structure

```
src/helion_fx_mlir/
├── __init__.py              # Public API exports
├── helion_mlir.py           # Main entry point: generate_mlir()
├── ir_visitor.py            # IRVisitor: walks FX graphs and dispatches to registry
├── mlir_builder.py          # MLIR text emission utilities
├── lowering_context.py      # Lowering state management
├── op_registry.py           # Registry for lowering implementations
├── torch_mlir_helper.py     # Wrapper for torch-mlir FxImporter (torch dialect)
└── lowerings/               # Modular lowering implementations
    ├── __init__.py          # Auto-registration of lowerings
    ├── base.py              # Base classes for lowerings
    ├── aten_ops.py          # ATen op lowerings (placeholder, uses torch-mlir)
    ├── memory_ops.py        # helion.load/store
    ├── control_flow.py      # affine.for/parallel
    └── ...
```

---

## Core Modules & Data Flow

### 1. `helion_mlir.py` - Main Entry Point

**Purpose**: Entry point for MLIR generation.

### 2. `ir_visitor.py` - Graph Walker

**Purpose**: Walks Device IR FX graphs node-by-node.

**Responsibilities**:
- Dispatch to `LoweringRegistry` if a lowering is registered (mostly for ATen ops using `torch-mlir`).
- Fall back to internal handler methods for Helion-specific nodes (`load`, `store`, `_for_loop`).
- Coordinate the generation of mixed dialect MLIR.

### 3. `torch_mlir_helper.py`

**Purpose**: Integration point for `torch-mlir`.

**Functionality**:
- Uses `torch-mlir`'s `FxImporter` to convert ATen nodes to MLIR.
- Configured to emit **torch dialect** (`torch.aten.*`) ops by default.
- The torch dialect can optionally be lowered to `linalg-on-tensors` using `lower_mlir_module()` or `PassManager` pipelines.
- This allows Helion to support a wide range of PyTorch operators without manually implementing lowerings for each one.

**Key APIs from torch-mlir**:
- `lower_mlir_module(verbose, output_type, module)` - Higher-level lowering API
- `run_pipeline_with_repro_report(module, pipeline, description)` - Run MLIR pass pipelines
- Pipelines: `torch-backend-to-linalg-on-tensors-backend-pipeline`, `torch-backend-to-tosa-backend-pipeline`, `torch-backend-to-stablehlo-backend-pipeline`

### Complete Lowering Flow

```
┌─────────────────────────────────────────────────────────────────────┐
│ 1. INPUT: BoundKernel                                               │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 2. CREATE CONTEXT: LoweringContext                                  │
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 3. IRVisitor: Walk Graph                                            │
│    For each FX node:                                                │
│      a. Check if ATen op -> use torch-mlir to emit torch dialect    │
│      b. Helion ops -> dispatch to visit_* methods (e.g., visit_load)│
└─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────────┐
│ 4. EMISSION                                                         │
│    - Builder emits MLIR text (helion.*, torch.aten.*, affine.*)     │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Op Mapping Reference (Generators)

| Device IR Node | Generator | Resulting Dialect/Op |
|----------------|-----------|----------------------|
| `aten.addmm` | **torch-mlir** | `torch.aten.addmm` |
| `aten.bmm` | **torch-mlir** | `torch.aten.bmm` |
| `aten.exp` | **torch-mlir** | `torch.aten.exp` |
| `aten.div.Tensor` | **torch-mlir** | `torch.aten.div.Tensor` |
| `load` | **this project** | `helion.load` |
| `store` | **this project** | `helion.store` |
| `_for_loop` | **this project** | `affine.for` |
| `_phi` | **this project** | `helion.phi` |
| `full` | **this project** | `helion.full` |

---

## Extending the System

### Adding a New Op Handler

Use the `@register_lowering` decorator. If it's a standard ATen op, you can often use the `TorchMLIRNodeImporter` to handle it automatically via `torch-mlir`.

```python
from .op_registry import register_lowering
from .lowerings.base import MLIRLowering
import torch

@register_lowering(torch.ops.aten.add.Tensor)
class AddLowering(MLIRLowering):
    def emit(self, ctx, node):
        # Implementation relying on torch-mlir helper
        ...
```
